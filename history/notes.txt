2017-05-09:

1. testing mAP 0.45, training mAP 0.63
2. fine tuning from 90k to 12k, testig maP 0.49 , trainin mAP 0.67
3. small objects like bottle(5), chair(9) has low mAP (0.1), probably because our data augmentation has been doing zoom in, and yet these objects are small in the image. data augmentation does not reflect both testing and training dataset.

2017-05-10:

1. Remvoe random crop, random color, retain left/right clip only, train 40k. 
testing mAP 0.27, training mAP 0.83, loss, 3.4, positive number 550, cross entroy_neg = 238, cross_entropy_pos=633,localization 346.
2. fine tune vgg16 weights, 20k

2017-05-11:
1. 20k fine tuning last about 12 hours, testing mAP 0.25, training mAP 0.83.  loss 3.1, positive number 550,cross_entory_neg=132,cross_entropy_pos=406,localization=216. weight do not seem to change during this 20k fine tuning. training does not seem go well, maybe it has to do with learning rate, batch normalization might help. or there might be bugs in codes.
2. add romdom crop, area [0.8,1], max_attempts=2,bbox_crop_overlap = 0.8, 40k steps
training time 12hours, testing mAP 0.37, training mAP 0.78, loss 4.6, postive number= 500, cross_entropy_neg=499,  cross_entropy_pos=972,localization=720.

2017-05-12:
1. add batch normalization layers into ssd network
testing mAP 0.39, training mAP 0.75, pos_num=500,loss=4.9, neg=359, pos=930,loc=642. in all, we see slight improvement after adding batch normalization.
2.tried fine tuning vgg weights from 1k, found that v66g weights don't change at all
first it's because learning rate being too small (0.0001), causeing both ssd and vgg16 weights do not change. after increasing learning rate to 0.001, ssd weights start, but vgg16 still don't change, this is probably becasue the netwrok being too deep (23 layers in all). considering vgg16 weights is already trained on similar dataset (imagenet), there is not much need to update vgg16 weigths
3. increase learning rate from 0.001 to 0.01, ssd weights update only
training time, 9 hours, testing mAP 0.44 (category 5 and categoy 16 has particularly low score, 0.1), training mAP 0.86. loss=2.9, neg=270, pos=470,loc=329. 

2017-05-13:
1. change romdom crop from area [0.8,1] to [0.6,1]
testing mAP=0.47, training mAP=0.84, loss=3.8,neg=282,pos=600,loc=400

2017-05-14:
1. use learning rate decay, decay learning rate by half every 5k till it goes down to 0.001. worse result.
testing mAP=0.45, training mAP=0.78, loss=4.3,neg=273,pos=665,loc=412

2017-05-21:
1. add 0.5 dropout, 140k,
testing mAP=0.47, training mAP = 0.80.

2017-05-22:
1. no random crop, color distortion. learning rate 0.1, 40k training step.

testing mAP 0.45, trainin mAP 0.87. reached 0.85 with only 5k trainin step. category 5,9,16 has only 0.1 testing mAP, while category 8 has the highest 0.7.  training loss 2.8 (5k), 1.6 (40k)

2017-05-23:
1. check training image fetch,

for one epoch, 5011 training images, 4458 are fetched, the max is 2.
for two epoch, 5011 training images, 5010 are fetched, the max is 3
for four epoch, 5011 training images, 5011 are fetched, the min is 2, the max is 5
for eight epoch, 5011 training images, 5011 are fetchd, the min is 6, the max is 9

2. reduce common_queue_capacity from 30 to 20

for one epoch, 5011 training image, 4505 are fetched, the max is 2.
for eights epochs, 5011 trainig images, 50111 are fetched, the min is 7 , the max is 9 

3. check boudning box 

1, number of matched image 240 matched bboxes 331, ratio 1.3791666666666667
2, number of matched image 255 matched bboxes 418, ratio 1.6392156862745098
3, number of matched image 334 matched bboxes 600, ratio 1.7964071856287425
4, number of matched image 188 matched bboxes 398, ratio 2.117021276595745
5, number of matched image 262 matched bboxes 634, ratio 2.4198473282442747
6, number of matched image 197 matched bboxes 272, ratio 1.380710659898477
7, number of matched image 765 matched bboxes 1651, ratio 2.1581699346405228
8, number of matched image 345 matched bboxes 390, ratio 1.1304347826086956
9, number of matched image 575 matched bboxes 1436, ratio 2.497391304347826
10, number of matched image 147 matched bboxes 357, ratio 2.4285714285714284
11, number of matched image 263 matched bboxes 310, ratio 1.1787072243346008
12, number of matched image 430 matched bboxes 538, ratio 1.2511627906976743
13, number of matched image 296 matched bboxes 411, ratio 1.3885135135135136
14, number of matched image 249 matched bboxes 390, ratio 1.5662650602409638
15, number of matched image 2100 matched bboxes 5463, ratio 2.6014285714285714
16, number of matched image 273 matched bboxes 625, ratio 2.2893772893772892
17, number of matched image 97 matched bboxes 353, ratio 3.6391752577319587
18, number of matched image 373 matched bboxes 426, ratio 1.1420911528150135
19, number of matched image 264 matched bboxes 330, ratio 1.25
20, number of matched image 279 matched bboxes 367, ratio 1.3154121863799284

2012-05-24:
1. learning rate 0.1 10k, then fine tune, learning rate 0.01, 45k.

by 10k, loss 2.8, by 45k, loss 1.0
by 10k, testing 0.44, training 0.86. by 45k, 0.97

for category 5, testing remains about 0.1 since 1k, training rise from 0.60 to 0.93

fine tune can greatly improve training mAP, training loss.

2017-05-26:
1. add training dataset 12
1)for one trainig epoch, 22136 training samples in all, 21200 are fetching , so fetching is ok.
2) bounding boxes statistics,
1, number of matched image 956 matched bboxes 1333, ratio 1.3943514644351465
2, number of matched image 858 matched bboxes 1255, ratio 1.4627039627039626
3, number of matched image 1144 matched bboxes 1870, ratio 1.6346153846153846
4, number of matched image 737 matched bboxes 1457, ratio 1.976933514246947
5, number of matched image 1074 matched bboxes 2195, ratio 2.043761638733706
6, number of matched image 664 matched bboxes 957, ratio 1.4412650602409638
7, number of matched image 2046 matched bboxes 4137, ratio 2.0219941348973607
8, number of matched image 1473 matched bboxes 1667, ratio 1.131704005431093
9, number of matched image 1940 matched bboxes 4492, ratio 2.3154639175257734
10, number of matched image 486 matched bboxes 1127, ratio 2.3189300411522633
11, number of matched image 954 matched bboxes 1110, ratio 1.1635220125786163
12, number of matched image 1771 matched bboxes 2136, ratio 1.2060982495765105
13, number of matched image 820 matched bboxes 1209, ratio 1.474390243902439
14, number of matched image 824 matched bboxes 1191, ratio 1.4453883495145632
15, number of matched image 11678 matched bboxes 22848, ratio 1.9564994005822915
16, number of matched image 886 matched bboxes 1827, ratio 2.0620767494356658
17, number of matched image 454 matched bboxes 1437, ratio 3.1651982378854626
18, number of matched image 1114 matched bboxes 1266, ratio 1.1364452423698383
19, number of matched image 852 matched bboxes 1032, ratio 1.2112676056338028
20, number of matched image 925 matched bboxes 1261, ratio 1.3632432432432433

2017-05-37:
20k, training, 0.1 learning rate, loss 3.1, testing 
80k, fine tuning, 0.05 training rate.loss 1.4, testing 0.57, training 0.97

2017-05-29:
30k, training, learning rate 0.1, testing 0.50, training  0.84
 
90k, fine tuning, 0.01, testing 0.54, training 0.92. loss, 1.5

2017-06-02:

added color distortion,

30k, training, learning rate 0.1, testing 0.5, training 0.80,loss
90k, fine tuning, learning 0.02,  testing 0.56, trainging 0.96  (highest 0.58, 0.93, 61k)

added strong patch sampling

30k, training, learning rate 0.1, mAP 0.49, 0.55, loss, 4.6
60k, fine tuning, learning rate 0.02, mAP 0.59, 0.68,loss,3.5
90k, fine tuning, learning rate 0.01, mAP 0.63, 0.77,loss,3.1
120, fine tuning, learning rate 0.001,mAP 0.65, 0.80,loss,2.6


140, fine tuning, learning rate 0.01,mAP  0.62, 0.78, loss  2.8,  0.01 too big. result deteriorates. 

130, fine tuning, learning rate 0.005,mAP  0.62,0.79, loss 2.7
130, fine tuning, learning rate 0.001, maP, 0.65,0.81, loss 2.6  140, mAP 0.66,0.81, loss 2.6
130, fine tuning, learning rate 0.0005 mAP,  0.66,0.81

130, fine tuning, learning rate 0.0001 mAP,  0.66,0.81, 140, learning rate 0.0001, mAP 0.66, 0.81

In summary, after 100k, not much improvement at all.

2017-06-10:

Add dropout 0.8, aspect ratio to [0.5,2]

30k, training, learning rate 0.1, mAP 0.46,0.49, (40k, 0.46,.0.49, not used)loss, 5.1

60k, fine tuning, learning rate 0.01, mAP  0.56,0.63   loss 3.9
90k, fine tuning, learning rate 0.01, mAP  0.59,0.69, loss, 3.6

100k, fine tuning, learning rate 0.001,mAP  0.65,0.74, loss,3.8
110k, fine tuning, learning rate 0.001,mAP   0.64,.75,loss ,3.4
120k, fine tuning, learning rate 0.0005 mAP, 0.65,0.76, loss 3.4





