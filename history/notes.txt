2017-05-09:

1. testing mAP 0.45, training mAP 0.63
2. fine tuning from 90k to 12k, testig maP 0.49 , trainin mAP 0.67
3. small objects like bottle(5), chair(9) has low mAP (0.1), probably because our data augmentation has been doing zoom in, and yet these objects are small in the image. data augmentation does not reflect both testing and training dataset.

2017-05-10:

1. Remvoe random crop, random color, retain left/right clip only, train 40k. 
testing mAP 0.27, training mAP 0.83, loss, 3.4, positive number 550, cross entroy_neg = 238, cross_entropy_pos=633,localization 346.
2. fine tune vgg16 weights, 20k

2017-05-11:
1. 20k fine tuning last about 12 hours, testing mAP 0.25, training mAP 0.83.  loss 3.1, positive number 550,cross_entory_neg=132,cross_entropy_pos=406,localization=216. weight do not seem to change during this 20k fine tuning. training does not seem go well, maybe it has to do with learning rate, batch normalization might help. or there might be bugs in codes.
2. add romdom crop, area [0.8,1], max_attempts=2,bbox_crop_overlap = 0.8, 40k steps
training time 12hours, testing mAP 0.37, training mAP 0.78, loss 4.6, postive number= 500, cross_entropy_neg=499,  cross_entropy_pos=972,localization=720.

2017-05-12:
1. add batch normalization layers into ssd network
testing mAP 0.39, training mAP 0.75, pos_num=500,loss=4.9, neg=359, pos=930,loc=642. in all, we see slight improvement after adding batch normalization.
2.tried fine tuning vgg weights from 1k, found that v66g weights don't change at all
first it's because learning rate being too small (0.0001), causeing both ssd and vgg16 weights do not change. after increasing learning rate to 0.001, ssd weights start, but vgg16 still don't change, this is probably becasue the netwrok being too deep (23 layers in all). considering vgg16 weights is already trained on similar dataset (imagenet), there is not much need to update vgg16 weigths
3. increase learning rate from 0.001 to 0.01, ssd weights update only
training time, 9 hours, testing mAP 0.44 (category 5 and categoy 16 has particularly low score, 0.1), training mAP 0.86. loss=2.9, neg=270, pos=470,loc=329. 

2017-05-14:
1. change romdom crop from area [0.8,1] to [0.6,1]
testing mAP=0.47, training mAP=0.84, loss=3.8,neg=282,pos=600,loc=400

2017-05-15:
1. use learning rate decay, decay learning rate by half every 5k till it goes down to 0.001. worse result.
testing mAP=0.45, training mAP=0.78, loss=4.3,neg=273,pos=665,loc=412



